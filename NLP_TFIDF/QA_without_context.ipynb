{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Les paths de fichiers\n",
    "path1 = r\"C:\\Users\\Zakaria-Laptop\\LLM-Models\\NLP_TFIDF\\DataBase\\S08_question_answer_pairs.txt\"\n",
    "path2 = r\"C:\\Users\\Zakaria-Laptop\\LLM-Models\\NLP_TFIDF\\DataBase\\S09_question_answer_pairs.txt\"\n",
    "path3 = r\"C:\\Users\\Zakaria-Laptop\\LLM-Models\\NLP_TFIDF\\DataBase\\S10_question_answer_pairs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>DifficultyFromQuestioner</th>\n",
       "      <th>DifficultyFromAnswerer</th>\n",
       "      <th>ArticleFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abraham_Lincoln</td>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S08_set3_a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abraham_Lincoln</td>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>Yes.</td>\n",
       "      <td>easy</td>\n",
       "      <td>easy</td>\n",
       "      <td>S08_set3_a4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleTitle                                           Question Answer  \\\n",
       "0  Abraham_Lincoln  Was Abraham Lincoln the sixteenth President of...    yes   \n",
       "1  Abraham_Lincoln  Was Abraham Lincoln the sixteenth President of...   Yes.   \n",
       "\n",
       "  DifficultyFromQuestioner DifficultyFromAnswerer  ArticleFile  \n",
       "0                     easy                   easy  S08_set3_a4  \n",
       "1                     easy                   easy  S08_set3_a4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"    \n",
    "    return pd.read_csv(path, sep=\"\t\")\n",
    "\n",
    "data1 = read_file(path1)\n",
    "data2 = read_file(path2)\n",
    "# It's a file encoded with ISO\n",
    "data3 = pd.read_csv(path3, sep=\"\\t\", encoding='ISO-8859-1')\n",
    "\n",
    "data = pd.concat([data1, data2, data3])\n",
    "data.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ArticleTitle', 'Question', 'Answer', 'DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'],\n",
       "    num_rows: 3998\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_data = data.copy()\n",
    "# Transformer en un Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(copy_data)\n",
    "dataset = dataset.remove_columns(\"__index_level_0__\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Question', 'Answer'],\n",
      "        num_rows: 3998\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from datasets import Dataset, DatasetDict\n",
    "\n",
    "#Diviser les données en ensembles train, validation, test\n",
    "train_df = copy_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Créer les ensembles DataSet\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "# Créer les DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "})\n",
    "dataset_dict = dataset_dict.remove_columns('__index_level_0__')\n",
    "print(dataset_dict)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Question', 'Answer'],\n",
      "    num_rows: 3998\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "#Diviser les données en ensembles train, validation, test\n",
    "train_df = copy_data.sample(frac=1, random_state=42)\n",
    "remaining_df = copy_data.drop(train_df.index)\n",
    "validation_df = remaining_df.sample(frac=0.25, random_state=42)\n",
    "test_df = remaining_df.drop(validation_df.index)\n",
    "\n",
    "# Créer les ensembles DataSet\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Créer les DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "dataset_dict = dataset_dict.remove_columns(['__index_level_0__', 'ArticleTitle', 'DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'])\n",
    "data_finale = dataset_dict[\"train\"]\n",
    "print(data_finale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Théorie:\n",
    "\n",
    "Pour créer un modèle qui fonctionne comme une FAQ, l'approche la plus courante consiste à utiliser des techniques de recherche de similarité de texte. Cela consiste à identifier quelle question de ton ensemble de données est la plus similaire à la question posée en entrée, puis à renvoyer la réponse correspondante.\n",
    "\n",
    "## Les étapes:\n",
    "\n",
    "__1 - Nettoyage et prétraitement des données :__\n",
    "\n",
    "Nettoyer les questions et réponses (enlever les stop words, la ponctuation, etc.).\n",
    "\n",
    "__2 - Vectorisation des questions :__\n",
    "\n",
    "Convertir les questions en vecteurs numériques. Une méthode classique consiste à utiliser des techniques comme TF-IDF, Word2Vec ou des modèles plus avancés comme les embeddings BERT (Bidirectional Encoder Representations from Transformers) pour obtenir des représentations vectorielles des questions.\n",
    "\n",
    "__3 - Calcul de la similarité :__\n",
    "\n",
    "Utiliser une mesure de similarité, comme la similarité cosinus, pour comparer la question en entrée à toutes les questions de ton ensemble de données et identifier la question la plus proche.\n",
    "\n",
    "__4 - Récupération de la réponse :__\n",
    "\n",
    "Renvoyer la réponse correspondant à la question la plus similaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche TF-IDF et Cos-similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les None de ma base de données vu que lors de l'application de vectorizer.fit_transform ça applique .lower() et avec des None ça retoure des erreurs\n",
    "data = [item for item in data_finale if item[\"Question\"] is not None and item[\"Question\"].strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of questions is: 3961 and len of answers is: 3961\n"
     ]
    }
   ],
   "source": [
    "questions = [item[\"Question\"] for item in data]\n",
    "responses = [item[\"Answer\"] for item in data]\n",
    "print(f\"len of questions is: {len(questions)} and len of answers is: {len(responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape1: Véctorisation les questions avec TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir une fonction pour retourner la réponse basant sur le Cos-similarity\n",
    "\n",
    "def get_answer(question):\n",
    "    # Avoir le vector de la question\n",
    "    question_tfidf = vectorizer.transform([question])\n",
    "\n",
    "    # Appliquer le cos similarity avec ma matrice\n",
    "    similarity = cosine_similarity(question_tfidf, tf_idf_matrix).flatten()\n",
    "\n",
    "    # Trouver l'index de la question la plus similaire\n",
    "    closest_question_index = np.argmax(similarity)\n",
    "\n",
    "    # Retourner la réponse associe à cette question\n",
    "    return responses[closest_question_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many cymbals are usually included in a drum kit?\n",
      "Réponse: at least 3\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "new_question = \"How many cymbals are usually included in a drum kit?\"\n",
    "response = get_answer(new_question)\n",
    "print(f\"Question: {new_question}\\nRéponse: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------- To be continued -------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dash = \"-\".join(\"\" for x in range(50))\n",
    "print(f\"{dash} To be continued {dash}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
