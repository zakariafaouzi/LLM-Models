{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Task:\n",
    "\n",
    "La traduction est une t√¢che de type s√©quence-√†-s√©quence, similaire au r√©sum√© de texte. Elle peut √©galement √™tre adapt√©e √† d'autres probl√®mes de ce genre, comme le transfert de style (par exemple, traduire un texte formel en un texte plus d√©contract√©) ou la g√©n√©ration de r√©ponses √† des questions en fonction d'un contexte.\n",
    "\n",
    "Si vous disposez d'un grand corpus de textes dans deux langues ou plus, vous pouvez entra√Æner un nouveau mod√®le de traduction √† partir de z√©ro. Toutefois, il est souvent plus rapide de faire un fine-tuning d‚Äôun mod√®le de traduction existant, comme un mod√®le multilingue tel que mT5 ou mBART, ou un mod√®le sp√©cialis√© pour la traduction d‚Äôune langue √† une autre.\n",
    "\n",
    "Dans cette section, un mod√®le Marian pr√©-entra√Æn√© pour la traduction de l‚Äôanglais vers le fran√ßais sera ajust√© (fine-tuned) en utilisant le jeu de donn√©es KDE4. Ce mod√®le a √©t√© initialement entra√Æn√© sur un large corpus de textes en anglais et en fran√ßais, et nous allons am√©liorer ses performances apr√®s l‚Äô√©tape de fine-tuning.\n",
    "\n",
    "Une fois l‚Äôentra√Ænement termin√©, le mod√®le pourra faire des pr√©dictions de traduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Preparing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1454: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lire our dataset\n",
    "dataset = load_dataset(\"kde4\", lang1 = \"en\", lang2 = \"fr\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons 210 173 paires de phrases, mais elles sont regroup√©es en un seul ensemble, ce qui signifie que nous devons cr√©er notre propre ensemble de validation. Un objet Dataset poss√®de une m√©thode train_test_split() qui peut nous aider. Nous allons fournir une graine (seed) pour garantir la reproductibilit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitter our data in train and test\n",
    "split_datasets = dataset[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "\n",
    "#Rename our test to \"validation\"\n",
    "split_datasets[\"validation\"] = split_datasets.pop('test')\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Text Cursor Movement', 'fr': 'Mouvements du curseur de texte'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][10][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un dictionnaire contenant deux phrases dans les langues demand√©es. Une particularit√© de ce jeu de donn√©es, qui contient beaucoup de termes techniques en informatique, est que tous ces termes sont enti√®rement traduits en fran√ßais. Cependant, les ing√©nieurs fran√ßais laissent souvent les mots sp√©cifiques √† l'informatique en anglais lorsqu'ils parlent. Par exemple, le mot ¬´ threads ¬ª pourrait appara√Ætre tel quel dans une phrase fran√ßaise, surtout dans une conversation technique, mais dans ce jeu de donn√©es, il est traduit par l'expression plus correcte ¬´ fils de discussion ¬ª. Le mod√®le pr√©-entra√Æn√© que nous utilisons, qui a √©t√© form√© sur un corpus plus large de phrases en fran√ßais et en anglais, choisit souvent l'option plus simple en laissant le mot tel quel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Par d√©faut pour les threads √©largis'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre exemple de ce comportement est le mot ¬´ plugin ¬ª, qui n'est pas officiellement un mot fran√ßais mais que la plupart des francophones comprennent sans le traduire. Dans le jeu de donn√©es KDE4, ce mot a √©t√© traduit en fran√ßais par l'expression plus officielle ¬´ module d'extension ¬ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
       " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(split_datasets[\"train\"][172][\"translation\"][\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Processing the data\n",
    "\n",
    "les textes doivent √™tre convertis en ensembles d'ID de tokens pour que le mod√®le puisse les comprendre. Pour cette t√¢che, nous devons tokeniser √† la fois les entr√©es et les cibles. Notre premi√®re √©tape est de cr√©er l'objet tokenizer.\n",
    "\n",
    "Comme mentionn√© pr√©c√©demment, nous utiliserons un mod√®le Marian pr√©-entra√Æn√© pour la traduction de l'anglais vers le fran√ßais. Si vous utilisez ce code avec une autre paire de langues, veillez √† adapter le point de contr√¥le du mod√®le. L'organisation Helsinki-NLP propose plus d'un millier de mod√®les dans plusieurs langues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pr√©paration de nos donn√©es est assez simple. Il y a juste une chose √† retenir : il faut s'assurer que le tokenizer traite les cibles dans la langue de sortie (ici, le fran√ßais). Vous pouvez le faire en passant les cibles √† l'argument __text_targets__ de la m√©thode __call__ du tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [12406, 4, 9432, 26, 29464, 746, 24, 1637, 212, 28, 479, 3, 443, 10042, 24, 63, 2959, 517, 28, 32, 15108, 2, 4, 9432, 32, 801, 26, 1265, 9929, 246, 3, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [40276, 34, 3404, 5, 2099, 27, 19, 14776, 24, 3432, 92, 167, 23, 15, 102, 350, 14, 6, 9313, 153, 402, 29033, 15080, 402, 29033, 416, 43, 806, 17598, 2, 19, 3404, 5, 2099, 81, 6, 82, 5644, 29, 27, 16, 1871, 20, 6, 28349, 3, 0]}\n"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][120][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][120][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target = fr_sentence)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le constater, la sortie contient les IDs d'entr√©e associ√©s √† la phrase en anglais, tandis que les IDs associ√©s √† la phrase en fran√ßais sont stock√©s dans le champ des labels. Si vous oubliez de pr√©ciser que vous √™tes en train de tokeniser des labels, ils seront trait√©s par le tokenizer des entr√©es, ce qui, dans le cas d'un mod√®le Marian, ne fonctionnera pas du tout correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅSai', 's', 'isse', 'z', '‚ñÅun', '‚ñÅmo', 't', '‚ñÅde', '‚ñÅpass', 'e', '‚ñÅpour', '‚ñÅle', '‚ñÅd√©', 'mar', 'rage', '‚ñÅ(', 'si', '‚ñÅil', '‚ñÅy', '‚ñÅen', '‚ñÅa', ').', '‚ñÅSi', '‚ñÅl', \"'\", 'option', '‚ñÅ¬´', '‚ñÅ&', '‚ñÅ#1', '60', ';', '‚ñÅrest', 're', 'int', '‚ñÅ&', '‚ñÅ#1', '60', ';', '‚ñÅ¬ª', '‚ñÅest', '‚ñÅco', 'ch√©', 'e', ',', '‚ñÅle', '‚ñÅmo', 't', '‚ñÅde', '‚ñÅpass', 'e', '‚ñÅn', \"'\", 'est', '‚ñÅre', 'qui', 's', '‚ñÅque', '‚ñÅpour', '‚ñÅles', '‚ñÅchange', 'ments', '‚ñÅd', \"'\", 'option', 's', '.', '</s>']\n",
      "['‚ñÅSaisissez', '‚ñÅun', '‚ñÅmot', '‚ñÅde', '‚ñÅpasse', '‚ñÅpour', '‚ñÅle', '‚ñÅd√©marrage', '‚ñÅ(', 'si', '‚ñÅil', '‚ñÅy', '‚ñÅen', '‚ñÅa', ').', '‚ñÅSi', '‚ñÅl', \"'\", 'option', '‚ñÅ¬´', '‚ñÅ&', '‚ñÅ#160;', '‚ñÅrestreint', '‚ñÅ&', '‚ñÅ#160;', '‚ñÅ¬ª', '‚ñÅest', '‚ñÅco', 'ch√©e', ',', '‚ñÅle', '‚ñÅmot', '‚ñÅde', '‚ñÅpasse', '‚ñÅn', \"'\", 'est', '‚ñÅrequis', '‚ñÅque', '‚ñÅpour', '‚ñÅles', '‚ñÅchangements', '‚ñÅd', \"'\", 'options', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, utiliser le tokenizer anglais pour pr√©traiter une phrase en fran√ßais entra√Æne un nombre beaucoup plus √©lev√© de tokens, car le tokenizer ne conna√Æt pas les mots fran√ßais (sauf ceux qui apparaissent √©galement en anglais, comme ¬´ discussion ¬ª).\n",
    "\n",
    "Puisque les entr√©es sont un dictionnaire contenant nos cl√©s habituelles (IDs d'entr√©e, masque d'attention, etc.), la derni√®re √©tape consiste √† d√©finir la fonction de pr√©traitement que nous appliquerons aux jeux de donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(exemple: str):\n",
    "    \"\"\"une fct pour tokenize nos textes d'entr√©e et de sortie\n",
    "\n",
    "    Args:\n",
    "        exemple (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    inputs = [ex[\"en\"] for ex in exemple[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in exemple[\"translation\"]]\n",
    "\n",
    "    model_inputs = tokenizer(targets, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(preprocess_function, batched=True, \n",
    "                                        remove_columns=split_datasets[\"train\"].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Fine-tuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code r√©el utilisant le Trainer sera le m√™me que pr√©c√©demment, avec une petite diff√©rence : nous utilisons ici un $Seq2SeqTrainer$, qui est une sous-classe de Trainer permettant de g√©rer correctement l'√©valuation, en utilisant la m√©thode __generate()__ pour pr√©dire les sorties √† partir des entr√©es. Nous examinerons cela plus en d√©tail lorsque nous aborderons le calcul des m√©triques.\n",
    "\n",
    "Tout d'abord, nous avons besoin d'un mod√®le r√©el √† affiner. Nous utiliserons l'API AutoModel habituelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collation\n",
    "\n",
    "Nous aurons besoin d'un data collator pour g√©rer le padding lors du dynamic batching. Nous ne pouvons pas simplement utiliser un __DataCollatorWithPadding__, car celui-ci ne fait le padding que pour les entr√©es (IDs d'entr√©e, masque d'attention, et types de tokens). Nos labels doivent √©galement √™tre compl√©t√©s jusqu'√† la longueur maximale rencontr√©e. Comme mentionn√© pr√©c√©demment, la valeur de padding utilis√©e pour les labels doit √™tre -100 et non le token de padding du tokenizer, afin que ces valeurs soient ignor√©es lors du calcul de la perte.\n",
    "\n",
    "Tout cela est g√©r√© par un __DataCollatorForSeq2Seq__. Comme le __DataCollatorWithPadding__, il utilise le tokenizer pour pr√©traiter les entr√©es, mais il prend √©galement le mod√®le. En effet, ce collator est responsable de pr√©parer les IDs d'entr√©e du d√©codeur, qui sont des versions d√©cal√©es des labels avec un token sp√©cial au d√©but. √âtant donn√© que ce d√©calage varie selon les architectures, le __DataCollatorForSeq2Seq__ doit conna√Ætre l'objet mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Explication$\n",
    "\n",
    "__1. DataCollatorWithPadding :__\n",
    "\n",
    "Usage principal : G√©n√©ralement utilis√© pour les t√¢ches de classification, d'extraction d'information ou tout autre type de t√¢che o√π il est n√©cessaire de padder (remplir) les s√©quences pour qu'elles aient toutes la m√™me longueur dans un batch.\n",
    "\n",
    "Fonctionnement : Il ajuste la longueur des s√©quences de mani√®re dynamique au sein d'un batch, en ajoutant des tokens de padding ([PAD]) pour rendre chaque s√©quence de la m√™me longueur, sans toucher aux labels. Cela est particuli√®rement utile pour les mod√®les de type BERT, RoBERTa, etc., o√π les s√©quences d'entr√©e peuvent avoir des longueurs variables.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "S√©quences originales : [[1, 2, 3], [1, 2], [1]]\n",
    "Apr√®s padding : [[1, 2, 3], [1, 2, 0], [1, 0, 0]]\n",
    "\n",
    "__2. DataCollatorForSeq2Seq :__\n",
    "\n",
    "Usage principal : Sp√©cifiquement con√ßu pour les t√¢ches de g√©n√©ration de s√©quences comme la traduction ou le r√©sum√© de texte, o√π l'on traite des mod√®les Seq2Seq tels que T5, BART, etc.\n",
    "\n",
    "Fonctionnement : En plus de g√©rer le padding comme DataCollatorWithPadding, il g√®re √©galement les labels (les s√©quences cibles). Les labels doivent aussi √™tre padd√©s dans les t√¢ches Seq2Seq pour que toutes les s√©quences cibles aient la m√™me longueur dans un batch. De plus, il g√®re des aspects sp√©cifiques comme l'ignoration des tokens de padding dans les labels pour ne pas p√©naliser le mod√®le lorsqu'il pr√©dit ces tokens lors de l'entra√Ænement.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "S√©quences d'entr√©e : [[1, 2, 3], [1, 2]]\n",
    "Labels (s√©quences cibles) : [[4, 5, 6], [4, 5]]\n",
    "Apr√®s padding (entr√©e et labels) :\n",
    "S√©quences d'entr√©e : [[1, 2, 3], [1, 2, 0]]\n",
    "Labels : [[4, 5, 6], [4, 5, -100]] (o√π -100 indique que le padding ne sera pas pris en compte dans la perte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.keys of {'input_ids': tensor([[  577,  1205,   483, 13077,     2,  1205,   517, 13926,  1588,    16,\n",
       "          3842,     9,     5,  1710,     0, 59513, 59513],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "          4274,  3534,   794,  7907, 24842,  3386,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0]]), 'decoder_input_ids': tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons √©galement examiner les IDs d'entr√©e du d√©codeur pour v√©rifier qu'ils sont des versions d√©cal√©es des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons passer ce __data_collator__ au __Seq2SeqTrainer__. Ensuite, examinons la m√©trique.\n",
    "\n",
    "Voici la traduction et le r√©sum√© :\n",
    "\n",
    "La m√©trique traditionnelle utilis√©e pour la traduction est le score BLEU, introduit dans un article de 2002 par Kishore Papineni et al. Le score BLEU √©value √† quel point les traductions sont proches de leurs labels. Il ne mesure pas l'intelligibilit√© ou la correction grammaticale des sorties g√©n√©r√©es par le mod√®le, mais applique des r√®gles statistiques pour v√©rifier que tous les mots des sorties g√©n√©r√©es apparaissent √©galement dans les cibles. De plus, des r√®gles p√©nalisent les r√©p√©titions de mots si elles ne sont pas pr√©sentes dans les cibles (afin d'√©viter que le mod√®le g√©n√®re des phrases comme ¬´ the the the ¬ª) et les phrases trop courtes (comme ¬´ the ¬ª).\n",
    "\n",
    "Une faiblesse du score BLEU est qu'il n√©cessite que le texte soit d√©j√† tokenis√©, ce qui complique la comparaison entre des mod√®les utilisant diff√©rents tokenizers. C'est pourquoi la m√©trique la plus couramment utilis√©e aujourd'hui pour √©valuer les mod√®les de traduction est SacreBLEU, qui corrige ce d√©faut (et d'autres) en standardisant l'√©tape de tokenisation. Pour utiliser cette m√©trique, nous devons d'abord installer la biblioth√®que SacreBLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\zakaria-laptop\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (2024.5.15)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zakaria-laptop\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (1.26.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\zakaria-laptop\\appdata\\roaming\\python\\python312\\site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\zakaria-laptop\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.8/3.8 MB 740.2 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 2.1/3.8 MB 798.9 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 2.4/3.8 MB 843.9 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.6/3.8 MB 843.5 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 3.1/3.8 MB 936.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-5.3.0 portalocker-2.10.1 sacrebleu-2.4.3 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2c8a6750bb43d7a60d63d4aab82c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette m√©trique prend des textes en tant qu'entr√©es et cibles. Elle est con√ßue pour accepter plusieurs cibles possibles, car il existe souvent plusieurs traductions acceptables d'une m√™me phrase. Le jeu de donn√©es que nous utilisons n'en fournit qu'une, mais dans le domaine du traitement du langage naturel (NLP), il n'est pas rare de trouver des jeux de donn√©es avec plusieurs phrases en tant que labels. Ainsi, les pr√©dictions doivent √™tre une liste de phrases, tandis que les r√©f√©rences doivent √™tre une liste de listes de phrases.\n",
    "\n",
    "Essayons un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990186,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.666666666666668, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 0.004086771438464067,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour passer des sorties du mod√®le aux textes utilisables par la m√©trique, nous allons utiliser la m√©thode __tokenizer.batch_decode()__. Il nous suffit de supprimer tous les -100 dans les labels (le tokenizer s'occupera automatiquement de faire de m√™me pour le token de padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(decoded_preds, decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is done, we are ready to fine-tune our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d487ab7f54b42e0a365a11ff3c5a016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"Helsinki-NLP/opus-mt-en-fr\",  # Nom du mod√®le pr√©-entra√Æn√© pour la traduction (anglais vers fran√ßais)\n",
    "    evaluation_strategy=\"no\",  # Pas d'√©valuation pendant l'entra√Ænement\n",
    "    save_strategy=\"epoch\",  # Sauvegarde le mod√®le √† la fin de chaque √©poque\n",
    "    learning_rate=2e-5,  # Taux d'apprentissage pour l'optimiseur\n",
    "    per_device_train_batch_size=32,  # Taille du lot pour l'entra√Ænement sur chaque appareil\n",
    "    per_device_eval_batch_size=64,  # Taille du lot pour l'√©valuation\n",
    "    weight_decay=0.01,  # Taux de d√©croissance du poids pour r√©gulariser le mod√®le\n",
    "    save_total_limit=3,  # Limite √† 3 le nombre total de mod√®les sauvegard√©s\n",
    "    num_train_epochs=3,  # Nombre total d'√©poques pour l'entra√Ænement\n",
    "    predict_with_generate=True,  # Utilise la m√©thode de g√©n√©ration pour les pr√©dictions\n",
    "    fp16=True,  # Active l'utilisation de la pr√©cision flottante 16 bits pour acc√©l√©rer l'entra√Ænement\n",
    "    push_to_hub=True,  # Pousse le mod√®le final vers Hugging Face Hub\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just pass everything to the __Seq2SeqTrainer__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model= model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer l'entra√Ænement, nous allons d'abord v√©rifier le score obtenu par notre mod√®le pour nous assurer que le fine-tuning n'aggrave pas les choses. Cette commande prendra un certain temps, alors vous pouvez en profiter pour prendre un caf√© pendant son ex√©cution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the __push_to_hub()__ method to make sure we upload the latest version of the model. The Trainer also drafts a model card with all the evaluation results and uploads it. This model card contains metadata that helps the Model Hub pick the widget for the inference demo. Usually, there is no need to say anything as it can infer the right widget from the model class, but in this case, the same model class can be used for all kinds of sequence-to-sequence problems, so we specify it‚Äôs a translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
