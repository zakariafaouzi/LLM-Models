{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Task:\n",
    "\n",
    "La traduction est une tâche de type séquence-à-séquence, similaire au résumé de texte. Elle peut également être adaptée à d'autres problèmes de ce genre, comme le transfert de style (par exemple, traduire un texte formel en un texte plus décontracté) ou la génération de réponses à des questions en fonction d'un contexte.\n",
    "\n",
    "Si vous disposez d'un grand corpus de textes dans deux langues ou plus, vous pouvez entraîner un nouveau modèle de traduction à partir de zéro. Toutefois, il est souvent plus rapide de faire un fine-tuning d’un modèle de traduction existant, comme un modèle multilingue tel que mT5 ou mBART, ou un modèle spécialisé pour la traduction d’une langue à une autre.\n",
    "\n",
    "Dans cette section, un modèle Marian pré-entraîné pour la traduction de l’anglais vers le français sera ajusté (fine-tuned) en utilisant le jeu de données KDE4. Ce modèle a été initialement entraîné sur un large corpus de textes en anglais et en français, et nous allons améliorer ses performances après l’étape de fine-tuning.\n",
    "\n",
    "Une fois l’entraînement terminé, le modèle pourra faire des prédictions de traduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Preparing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1454: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lire our dataset\n",
    "dataset = load_dataset(\"kde4\", lang1 = \"en\", lang2 = \"fr\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons 210 173 paires de phrases, mais elles sont regroupées en un seul ensemble, ce qui signifie que nous devons créer notre propre ensemble de validation. Un objet Dataset possède une méthode train_test_split() qui peut nous aider. Nous allons fournir une graine (seed) pour garantir la reproductibilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitter our data in train and test\n",
    "split_datasets = dataset[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "\n",
    "#Rename our test to \"validation\"\n",
    "split_datasets[\"validation\"] = split_datasets.pop('test')\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Text Cursor Movement', 'fr': 'Mouvements du curseur de texte'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][10][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un dictionnaire contenant deux phrases dans les langues demandées. Une particularité de ce jeu de données, qui contient beaucoup de termes techniques en informatique, est que tous ces termes sont entièrement traduits en français. Cependant, les ingénieurs français laissent souvent les mots spécifiques à l'informatique en anglais lorsqu'ils parlent. Par exemple, le mot « threads » pourrait apparaître tel quel dans une phrase française, surtout dans une conversation technique, mais dans ce jeu de données, il est traduit par l'expression plus correcte « fils de discussion ». Le modèle pré-entraîné que nous utilisons, qui a été formé sur un corpus plus large de phrases en français et en anglais, choisit souvent l'option plus simple en laissant le mot tel quel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78efaa4af38342a684c630249811bbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Zakaria-Laptop\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a9e747a36d489cbf385859d6642340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1ed8bd292042838e5797b2d30ecdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Par défaut pour les threads élargis'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre exemple de ce comportement est le mot « plugin », qui n'est pas officiellement un mot français mais que la plupart des francophones comprennent sans le traduire. Dans le jeu de données KDE4, ce mot a été traduit en français par l'expression plus officielle « module d'extension »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
       " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(split_datasets[\"train\"][172][\"translation\"][\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Processing the data\n",
    "\n",
    "les textes doivent être convertis en ensembles d'ID de tokens pour que le modèle puisse les comprendre. Pour cette tâche, nous devons tokeniser à la fois les entrées et les cibles. Notre première étape est de créer l'objet tokenizer.\n",
    "\n",
    "Comme mentionné précédemment, nous utiliserons un modèle Marian pré-entraîné pour la traduction de l'anglais vers le français. Si vous utilisez ce code avec une autre paire de langues, veillez à adapter le point de contrôle du modèle. L'organisation Helsinki-NLP propose plus d'un millier de modèles dans plusieurs langues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La préparation de nos données est assez simple. Il y a juste une chose à retenir : il faut s'assurer que le tokenizer traite les cibles dans la langue de sortie (ici, le français). Vous pouvez le faire en passant les cibles à l'argument __text_targets__ de la méthode __call__ du tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [12406, 4, 9432, 26, 29464, 746, 24, 1637, 212, 28, 479, 3, 443, 10042, 24, 63, 2959, 517, 28, 32, 15108, 2, 4, 9432, 32, 801, 26, 1265, 9929, 246, 3, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [40276, 34, 3404, 5, 2099, 27, 19, 14776, 24, 3432, 92, 167, 23, 15, 102, 350, 14, 6, 9313, 153, 402, 29033, 15080, 402, 29033, 416, 43, 806, 17598, 2, 19, 3404, 5, 2099, 81, 6, 82, 5644, 29, 27, 16, 1871, 20, 6, 28349, 3, 0]}\n"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][120][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][120][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target = fr_sentence)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le constater, la sortie contient les IDs d'entrée associés à la phrase en anglais, tandis que les IDs associés à la phrase en français sont stockés dans le champ des labels. Si vous oubliez de préciser que vous êtes en train de tokeniser des labels, ils seront traités par le tokenizer des entrées, ce qui, dans le cas d'un modèle Marian, ne fonctionnera pas du tout correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Sai', 's', 'isse', 'z', '▁un', '▁mo', 't', '▁de', '▁pass', 'e', '▁pour', '▁le', '▁dé', 'mar', 'rage', '▁(', 'si', '▁il', '▁y', '▁en', '▁a', ').', '▁Si', '▁l', \"'\", 'option', '▁«', '▁&', '▁#1', '60', ';', '▁rest', 're', 'int', '▁&', '▁#1', '60', ';', '▁»', '▁est', '▁co', 'ché', 'e', ',', '▁le', '▁mo', 't', '▁de', '▁pass', 'e', '▁n', \"'\", 'est', '▁re', 'qui', 's', '▁que', '▁pour', '▁les', '▁change', 'ments', '▁d', \"'\", 'option', 's', '.', '</s>']\n",
      "['▁Saisissez', '▁un', '▁mot', '▁de', '▁passe', '▁pour', '▁le', '▁démarrage', '▁(', 'si', '▁il', '▁y', '▁en', '▁a', ').', '▁Si', '▁l', \"'\", 'option', '▁«', '▁&', '▁#160;', '▁restreint', '▁&', '▁#160;', '▁»', '▁est', '▁co', 'chée', ',', '▁le', '▁mot', '▁de', '▁passe', '▁n', \"'\", 'est', '▁requis', '▁que', '▁pour', '▁les', '▁changements', '▁d', \"'\", 'options', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, utiliser le tokenizer anglais pour prétraiter une phrase en français entraîne un nombre beaucoup plus élevé de tokens, car le tokenizer ne connaît pas les mots français (sauf ceux qui apparaissent également en anglais, comme « discussion »).\n",
    "\n",
    "Puisque les entrées sont un dictionnaire contenant nos clés habituelles (IDs d'entrée, masque d'attention, etc.), la dernière étape consiste à définir la fonction de prétraitement que nous appliquerons aux jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(exemple: str):\n",
    "    \"\"\"une fct pour tokenize nos textes d'entrée et de sortie\n",
    "\n",
    "    Args:\n",
    "        exemple (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    inputs = [ex[\"en\"] for ex in exemple[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in exemple[\"translation\"]]\n",
    "\n",
    "    model_inputs = tokenizer(targets, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38edc6c81604c908b8670d5dbcb8f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268d430a0886494a93c43102c2e22422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21018 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(preprocess_function, batched=True, \n",
    "                                        remove_columns=split_datasets[\"train\"].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Fine-tuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code réel utilisant le Trainer sera le même que précédemment, avec une petite différence : nous utilisons ici un $Seq2SeqTrainer$, qui est une sous-classe de Trainer permettant de gérer correctement l'évaluation, en utilisant la méthode __generate()__ pour prédire les sorties à partir des entrées. Nous examinerons cela plus en détail lorsque nous aborderons le calcul des métriques.\n",
    "\n",
    "Tout d'abord, nous avons besoin d'un modèle réel à affiner. Nous utiliserons l'API AutoModel habituelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
