{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Task:\n",
    "\n",
    "La traduction est une tâche de type séquence-à-séquence, similaire au résumé de texte. Elle peut également être adaptée à d'autres problèmes de ce genre, comme le transfert de style (par exemple, traduire un texte formel en un texte plus décontracté) ou la génération de réponses à des questions en fonction d'un contexte.\n",
    "\n",
    "Si vous disposez d'un grand corpus de textes dans deux langues ou plus, vous pouvez entraîner un nouveau modèle de traduction à partir de zéro. Toutefois, il est souvent plus rapide de faire un fine-tuning d’un modèle de traduction existant, comme un modèle multilingue tel que mT5 ou mBART, ou un modèle spécialisé pour la traduction d’une langue à une autre.\n",
    "\n",
    "Dans cette section, un modèle Marian pré-entraîné pour la traduction de l’anglais vers le français sera ajusté (fine-tuned) en utilisant le jeu de données KDE4. Ce modèle a été initialement entraîné sur un large corpus de textes en anglais et en français, et nous allons améliorer ses performances après l’étape de fine-tuning.\n",
    "\n",
    "Une fois l’entraînement terminé, le modèle pourra faire des prédictions de traduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Preparing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1454: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lire our dataset\n",
    "dataset = load_dataset(\"kde4\", lang1 = \"en\", lang2 = \"fr\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons 210 173 paires de phrases, mais elles sont regroupées en un seul ensemble, ce qui signifie que nous devons créer notre propre ensemble de validation. Un objet Dataset possède une méthode train_test_split() qui peut nous aider. Nous allons fournir une graine (seed) pour garantir la reproductibilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitter our data in train and test\n",
    "split_datasets = dataset[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "\n",
    "#Rename our test to \"validation\"\n",
    "split_datasets[\"validation\"] = split_datasets.pop('test')\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Text Cursor Movement', 'fr': 'Mouvements du curseur de texte'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][10][\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un dictionnaire contenant deux phrases dans les langues demandées. Une particularité de ce jeu de données, qui contient beaucoup de termes techniques en informatique, est que tous ces termes sont entièrement traduits en français. Cependant, les ingénieurs français laissent souvent les mots spécifiques à l'informatique en anglais lorsqu'ils parlent. Par exemple, le mot « threads » pourrait apparaître tel quel dans une phrase française, surtout dans une conversation technique, mais dans ce jeu de données, il est traduit par l'expression plus correcte « fils de discussion ». Le modèle pré-entraîné que nous utilisons, qui a été formé sur un corpus plus large de phrases en français et en anglais, choisit souvent l'option plus simple en laissant le mot tel quel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\Zakaria-Laptop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Par défaut pour les threads élargis'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre exemple de ce comportement est le mot « plugin », qui n'est pas officiellement un mot français mais que la plupart des francophones comprennent sans le traduire. Dans le jeu de données KDE4, ce mot a été traduit en français par l'expression plus officielle « module d'extension »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
       " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(split_datasets[\"train\"][172][\"translation\"][\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Processing the data\n",
    "\n",
    "les textes doivent être convertis en ensembles d'ID de tokens pour que le modèle puisse les comprendre. Pour cette tâche, nous devons tokeniser à la fois les entrées et les cibles. Notre première étape est de créer l'objet tokenizer.\n",
    "\n",
    "Comme mentionné précédemment, nous utiliserons un modèle Marian pré-entraîné pour la traduction de l'anglais vers le français. Si vous utilisez ce code avec une autre paire de langues, veillez à adapter le point de contrôle du modèle. L'organisation Helsinki-NLP propose plus d'un millier de modèles dans plusieurs langues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La préparation de nos données est assez simple. Il y a juste une chose à retenir : il faut s'assurer que le tokenizer traite les cibles dans la langue de sortie (ici, le français). Vous pouvez le faire en passant les cibles à l'argument __text_targets__ de la méthode __call__ du tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [12406, 4, 9432, 26, 29464, 746, 24, 1637, 212, 28, 479, 3, 443, 10042, 24, 63, 2959, 517, 28, 32, 15108, 2, 4, 9432, 32, 801, 26, 1265, 9929, 246, 3, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [40276, 34, 3404, 5, 2099, 27, 19, 14776, 24, 3432, 92, 167, 23, 15, 102, 350, 14, 6, 9313, 153, 402, 29033, 15080, 402, 29033, 416, 43, 806, 17598, 2, 19, 3404, 5, 2099, 81, 6, 82, 5644, 29, 27, 16, 1871, 20, 6, 28349, 3, 0]}\n"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][120][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][120][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target = fr_sentence)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le constater, la sortie contient les IDs d'entrée associés à la phrase en anglais, tandis que les IDs associés à la phrase en français sont stockés dans le champ des labels. Si vous oubliez de préciser que vous êtes en train de tokeniser des labels, ils seront traités par le tokenizer des entrées, ce qui, dans le cas d'un modèle Marian, ne fonctionnera pas du tout correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Sai', 's', 'isse', 'z', '▁un', '▁mo', 't', '▁de', '▁pass', 'e', '▁pour', '▁le', '▁dé', 'mar', 'rage', '▁(', 'si', '▁il', '▁y', '▁en', '▁a', ').', '▁Si', '▁l', \"'\", 'option', '▁«', '▁&', '▁#1', '60', ';', '▁rest', 're', 'int', '▁&', '▁#1', '60', ';', '▁»', '▁est', '▁co', 'ché', 'e', ',', '▁le', '▁mo', 't', '▁de', '▁pass', 'e', '▁n', \"'\", 'est', '▁re', 'qui', 's', '▁que', '▁pour', '▁les', '▁change', 'ments', '▁d', \"'\", 'option', 's', '.', '</s>']\n",
      "['▁Saisissez', '▁un', '▁mot', '▁de', '▁passe', '▁pour', '▁le', '▁démarrage', '▁(', 'si', '▁il', '▁y', '▁en', '▁a', ').', '▁Si', '▁l', \"'\", 'option', '▁«', '▁&', '▁#160;', '▁restreint', '▁&', '▁#160;', '▁»', '▁est', '▁co', 'chée', ',', '▁le', '▁mot', '▁de', '▁passe', '▁n', \"'\", 'est', '▁requis', '▁que', '▁pour', '▁les', '▁changements', '▁d', \"'\", 'options', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, utiliser le tokenizer anglais pour prétraiter une phrase en français entraîne un nombre beaucoup plus élevé de tokens, car le tokenizer ne connaît pas les mots français (sauf ceux qui apparaissent également en anglais, comme « discussion »).\n",
    "\n",
    "Puisque les entrées sont un dictionnaire contenant nos clés habituelles (IDs d'entrée, masque d'attention, etc.), la dernière étape consiste à définir la fonction de prétraitement que nous appliquerons aux jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(exemple: str):\n",
    "    \"\"\"une fct pour tokenize nos textes d'entrée et de sortie\n",
    "\n",
    "    Args:\n",
    "        exemple (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    inputs = [ex[\"en\"] for ex in exemple[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in exemple[\"translation\"]]\n",
    "\n",
    "    model_inputs = tokenizer(targets, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(preprocess_function, batched=True, \n",
    "                                        remove_columns=split_datasets[\"train\"].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Fine-tuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code réel utilisant le Trainer sera le même que précédemment, avec une petite différence : nous utilisons ici un $Seq2SeqTrainer$, qui est une sous-classe de Trainer permettant de gérer correctement l'évaluation, en utilisant la méthode __generate()__ pour prédire les sorties à partir des entrées. Nous examinerons cela plus en détail lorsque nous aborderons le calcul des métriques.\n",
    "\n",
    "Tout d'abord, nous avons besoin d'un modèle réel à affiner. Nous utiliserons l'API AutoModel habituelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collation\n",
    "\n",
    "Nous aurons besoin d'un data collator pour gérer le padding lors du dynamic batching. Nous ne pouvons pas simplement utiliser un __DataCollatorWithPadding__, car celui-ci ne fait le padding que pour les entrées (IDs d'entrée, masque d'attention, et types de tokens). Nos labels doivent également être complétés jusqu'à la longueur maximale rencontrée. Comme mentionné précédemment, la valeur de padding utilisée pour les labels doit être -100 et non le token de padding du tokenizer, afin que ces valeurs soient ignorées lors du calcul de la perte.\n",
    "\n",
    "Tout cela est géré par un __DataCollatorForSeq2Seq__. Comme le __DataCollatorWithPadding__, il utilise le tokenizer pour prétraiter les entrées, mais il prend également le modèle. En effet, ce collator est responsable de préparer les IDs d'entrée du décodeur, qui sont des versions décalées des labels avec un token spécial au début. Étant donné que ce décalage varie selon les architectures, le __DataCollatorForSeq2Seq__ doit connaître l'objet modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Explication$\n",
    "\n",
    "__1. DataCollatorWithPadding :__\n",
    "\n",
    "Usage principal : Généralement utilisé pour les tâches de classification, d'extraction d'information ou tout autre type de tâche où il est nécessaire de padder (remplir) les séquences pour qu'elles aient toutes la même longueur dans un batch.\n",
    "\n",
    "Fonctionnement : Il ajuste la longueur des séquences de manière dynamique au sein d'un batch, en ajoutant des tokens de padding ([PAD]) pour rendre chaque séquence de la même longueur, sans toucher aux labels. Cela est particulièrement utile pour les modèles de type BERT, RoBERTa, etc., où les séquences d'entrée peuvent avoir des longueurs variables.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Séquences originales : [[1, 2, 3], [1, 2], [1]]\n",
    "Après padding : [[1, 2, 3], [1, 2, 0], [1, 0, 0]]\n",
    "\n",
    "__2. DataCollatorForSeq2Seq :__\n",
    "\n",
    "Usage principal : Spécifiquement conçu pour les tâches de génération de séquences comme la traduction ou le résumé de texte, où l'on traite des modèles Seq2Seq tels que T5, BART, etc.\n",
    "\n",
    "Fonctionnement : En plus de gérer le padding comme DataCollatorWithPadding, il gère également les labels (les séquences cibles). Les labels doivent aussi être paddés dans les tâches Seq2Seq pour que toutes les séquences cibles aient la même longueur dans un batch. De plus, il gère des aspects spécifiques comme l'ignoration des tokens de padding dans les labels pour ne pas pénaliser le modèle lorsqu'il prédit ces tokens lors de l'entraînement.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Séquences d'entrée : [[1, 2, 3], [1, 2]]\n",
    "Labels (séquences cibles) : [[4, 5, 6], [4, 5]]\n",
    "Après padding (entrée et labels) :\n",
    "Séquences d'entrée : [[1, 2, 3], [1, 2, 0]]\n",
    "Labels : [[4, 5, 6], [4, 5, -100]] (où -100 indique que le padding ne sera pas pris en compte dans la perte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.keys of {'input_ids': tensor([[  577,  1205,   483, 13077,     2,  1205,   517, 13926,  1588,    16,\n",
       "          3842,     9,     5,  1710,     0, 59513, 59513],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "          4274,  3534,   794,  7907, 24842,  3386,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0]]), 'decoder_input_ids': tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également examiner les IDs d'entrée du décodeur pour vérifier qu'ils sont des versions décalées des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons passer ce __data_collator__ au __Seq2SeqTrainer__. Ensuite, examinons la métrique.\n",
    "\n",
    "Voici la traduction et le résumé :\n",
    "\n",
    "La métrique traditionnelle utilisée pour la traduction est le score BLEU, introduit dans un article de 2002 par Kishore Papineni et al. Le score BLEU évalue à quel point les traductions sont proches de leurs labels. Il ne mesure pas l'intelligibilité ou la correction grammaticale des sorties générées par le modèle, mais applique des règles statistiques pour vérifier que tous les mots des sorties générées apparaissent également dans les cibles. De plus, des règles pénalisent les répétitions de mots si elles ne sont pas présentes dans les cibles (afin d'éviter que le modèle génère des phrases comme « the the the ») et les phrases trop courtes (comme « the »).\n",
    "\n",
    "Une faiblesse du score BLEU est qu'il nécessite que le texte soit déjà tokenisé, ce qui complique la comparaison entre des modèles utilisant différents tokenizers. C'est pourquoi la métrique la plus couramment utilisée aujourd'hui pour évaluer les modèles de traduction est SacreBLEU, qui corrige ce défaut (et d'autres) en standardisant l'étape de tokenisation. Pour utiliser cette métrique, nous devons d'abord installer la bibliothèque SacreBLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\zakaria-laptop\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (2024.5.15)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zakaria-laptop\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (1.26.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\zakaria-laptop\\appdata\\roaming\\python\\python312\\site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\zakaria-laptop\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.8/3.8 MB 740.2 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 2.1/3.8 MB 798.9 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 2.4/3.8 MB 843.9 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.6/3.8 MB 843.5 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 3.1/3.8 MB 936.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
      "Successfully installed lxml-5.3.0 portalocker-2.10.1 sacrebleu-2.4.3 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2c8a6750bb43d7a60d63d4aab82c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette métrique prend des textes en tant qu'entrées et cibles. Elle est conçue pour accepter plusieurs cibles possibles, car il existe souvent plusieurs traductions acceptables d'une même phrase. Le jeu de données que nous utilisons n'en fournit qu'une, mais dans le domaine du traitement du langage naturel (NLP), il n'est pas rare de trouver des jeux de données avec plusieurs phrases en tant que labels. Ainsi, les prédictions doivent être une liste de phrases, tandis que les références doivent être une liste de listes de phrases.\n",
    "\n",
    "Essayons un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990186,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.66666666666667,\n",
       "  54.54545454545455,\n",
       "  40.0,\n",
       "  33.333333333333336],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.666666666666668, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 0.004086771438464067,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour passer des sorties du modèle aux textes utilisables par la métrique, nous allons utiliser la méthode __tokenizer.batch_decode()__. Il nous suffit de supprimer tous les -100 dans les labels (le tokenizer s'occupera automatiquement de faire de même pour le token de padding)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
